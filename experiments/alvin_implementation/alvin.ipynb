{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4556d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ], (8,), float32)\n",
      "Action space: Box(-1.0, 1.0, (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('LunarLanderContinuous-v3')\n",
    "print(f\"Observation space: {env.observation_space}\")  # Box(8,)\n",
    "print(f\"Action space: {env.action_space}\")            # Box(2,)\n",
    "\n",
    "# State: [x, y, vx, vy, angle, angular_velocity, left_leg_contact, right_leg_contact]\n",
    "# Actions: [main_engine_power, side_engine_power] both in [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee06d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'hf_cache_home' from 'huggingface_hub.constants' (c:\\Users\\puyua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\constants.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion_policy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtedi_unet_lowdim_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TEDiUnetLowdimPolicy\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion_policy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconditional_unet1d_tedi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConditionalUnet1D\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion_policy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschedulers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDPMTEDiScheduler\n",
      "File \u001b[1;32mh:\\program\\mav-ei\\experiments\\alvin_implementation\\streaming_diffusion_policy\\diffusion_policy\\policy\\tedi_unet_lowdim_policy.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce, repeat\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion_policy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschedulers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDPMTEDiScheduler\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion_policy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearNormalizer\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion_policy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_lowdim_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLowdimPolicy\n",
      "File \u001b[1;32mh:\\program\\mav-ei\\experiments\\alvin_implementation\\streaming_diffusion_policy\\diffusion_policy\\policy\\schedulers.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigMixin, register_to_config\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _COMPATIBLE_STABLE_DIFFUSION_SCHEDULERS, BaseOutput, deprecate\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschedulers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscheduling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchedulerMixin\n",
      "File \u001b[1;32mc:\\Users\\puyua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.21.4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m      7\u001b[0m     _LazyModule,\n\u001b[0;32m      8\u001b[0m     is_flax_available,\n\u001b[0;32m      9\u001b[0m     is_k_diffusion_available,\n\u001b[0;32m     10\u001b[0m     is_librosa_available,\n\u001b[0;32m     11\u001b[0m     is_note_seq_available,\n\u001b[0;32m     12\u001b[0m     is_onnx_available,\n\u001b[0;32m     13\u001b[0m     is_scipy_available,\n\u001b[0;32m     14\u001b[0m     is_torch_available,\n\u001b[0;32m     15\u001b[0m     is_torchsde_available,\n\u001b[0;32m     16\u001b[0m     is_transformers_available,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Lazy Import based on\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# https://github.com/huggingface/transformers/blob/main/src/transformers/__init__.py\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# When adding a new object to this init, please add it to `_import_structure`. The `_import_structure` is a dictionary submodule to list of object names,\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# and is used to defer the actual importing for when the objects are requested.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# This way `import diffusers` provides the names in the namespace without actually importing anything (and especially none of the backends).\u001b[39;00m\n\u001b[0;32m     27\u001b[0m _import_structure \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_utils\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfigMixin\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     ],\n\u001b[0;32m     50\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\puyua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\utils\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     CONFIG_NAME,\n\u001b[0;32m     23\u001b[0m     DEPRECATED_REVISION_ARGS,\n\u001b[0;32m     24\u001b[0m     DIFFUSERS_CACHE,\n\u001b[0;32m     25\u001b[0m     DIFFUSERS_DYNAMIC_MODULE_NAME,\n\u001b[0;32m     26\u001b[0m     FLAX_WEIGHTS_NAME,\n\u001b[0;32m     27\u001b[0m     HF_MODULES_CACHE,\n\u001b[0;32m     28\u001b[0m     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n\u001b[0;32m     29\u001b[0m     ONNX_EXTERNAL_WEIGHTS_NAME,\n\u001b[0;32m     30\u001b[0m     ONNX_WEIGHTS_NAME,\n\u001b[0;32m     31\u001b[0m     SAFETENSORS_WEIGHTS_NAME,\n\u001b[0;32m     32\u001b[0m     WEIGHTS_NAME,\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecate\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m replace_example_docstring\n",
      "File \u001b[1;32mc:\\Users\\puyua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\utils\\constants.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HUGGINGFACE_HUB_CACHE, hf_cache_home\n\u001b[0;32m     19\u001b[0m default_cache_path \u001b[38;5;241m=\u001b[39m HUGGINGFACE_HUB_CACHE\n\u001b[0;32m     22\u001b[0m CONFIG_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'hf_cache_home' from 'huggingface_hub.constants' (c:\\Users\\puyua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\constants.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('streaming_diffusion_policy')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from diffusion_policy.policy.tedi_unet_lowdim_policy import TEDiUnetLowdimPolicy\n",
    "from diffusion_policy.model.diffusion.conditional_unet1d_tedi import ConditionalUnet1D\n",
    "from diffusion_policy.policy.schedulers import DDPMTEDiScheduler\n",
    "\n",
    "# Lunar Lander specifics\n",
    "obs_dim = 8           # LunarLander state dimension\n",
    "action_dim = 2        # [main_engine, side_engine]\n",
    "\n",
    "# Temporal parameters (keep these or adjust)\n",
    "horizon = 16          # Must be divisible by 4\n",
    "n_obs_steps = 2       # Recent observations to condition on\n",
    "n_action_steps = 4    # Actions to execute per prediction (reduced for LL)\n",
    "\n",
    "# Diffusion parameters\n",
    "num_train_timesteps = 100\n",
    "num_inference_steps = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d63787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collection\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "def collect_demonstrations(num_episodes=100, render=False):\n",
    "    \"\"\"\n",
    "    Collect expert demonstrations using heuristic controller or trained agent\n",
    "    \"\"\"\n",
    "    env = gym.make('LunarLanderContinuous-v2', render_mode='human' if render else None)\n",
    "    \n",
    "    episodes = []\n",
    "    successful_episodes = 0\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        episode_data = {\n",
    "            'observations': [],\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'dones': []\n",
    "        }\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        step_count = 0\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            # Use a simple heuristic or load a pretrained agent\n",
    "            # For now, use random actions (you should replace this!)\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            # Store transition\n",
    "            episode_data['observations'].append(obs)\n",
    "            episode_data['actions'].append(action)\n",
    "            \n",
    "            # Step environment\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            episode_data['rewards'].append(reward)\n",
    "            episode_data['dones'].append(done or truncated)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            if step_count > 1000:  # Prevent infinite episodes\n",
    "                break\n",
    "        \n",
    "        # Only keep successful episodes (reward > 200 is good landing)\n",
    "        if episode_reward > 200:\n",
    "            episodes.append({\n",
    "                'observations': np.array(episode_data['observations']),\n",
    "                'actions': np.array(episode_data['actions']),\n",
    "                'rewards': np.array(episode_data['rewards']),\n",
    "                'total_reward': episode_reward\n",
    "            })\n",
    "            successful_episodes += 1\n",
    "            print(f\"Episode {ep}: ✅ Reward = {episode_reward:.1f} (kept)\")\n",
    "        else:\n",
    "            print(f\"Episode {ep}: ❌ Reward = {episode_reward:.1f} (discarded)\")\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"\\n✅ Collected {successful_episodes} successful episodes\")\n",
    "    \n",
    "    # Save to disk\n",
    "    with open('lunar_lander_demos.pkl', 'wb') as f:\n",
    "        pickle.dump(episodes, f)\n",
    "    \n",
    "    return episodes\n",
    "\n",
    "# Uncomment to collect data:\n",
    "# episodes = collect_demonstrations(num_episodes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LunarLanderDataset(Dataset):\n",
    "    def __init__(self, episodes, horizon=16, n_obs_steps=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            episodes: List of episode dictionaries with 'observations' and 'actions'\n",
    "            horizon: Length of action sequence to predict\n",
    "            n_obs_steps: Number of observation steps for conditioning\n",
    "        \"\"\"\n",
    "        self.horizon = horizon\n",
    "        self.n_obs_steps = n_obs_steps\n",
    "        self.episodes = episodes\n",
    "        \n",
    "        # Create indices for sampling\n",
    "        self.indices = []\n",
    "        for ep_idx, episode in enumerate(episodes):\n",
    "            episode_len = len(episode['actions'])\n",
    "            # We need at least n_obs_steps + horizon\n",
    "            if episode_len >= n_obs_steps + horizon:\n",
    "                # Can sample from any position that has enough future steps\n",
    "                for start_idx in range(episode_len - horizon - n_obs_steps + 1):\n",
    "                    self.indices.append((ep_idx, start_idx))\n",
    "        \n",
    "        print(f\"Dataset created with {len(self.indices)} samples from {len(episodes)} episodes\")\n",
    "        \n",
    "        # Compute normalization statistics\n",
    "        all_obs = np.concatenate([ep['observations'] for ep in episodes])\n",
    "        all_actions = np.concatenate([ep['actions'] for ep in episodes])\n",
    "        \n",
    "        self.obs_mean = all_obs.mean(axis=0)\n",
    "        self.obs_std = all_obs.std(axis=0) + 1e-8\n",
    "        self.action_mean = all_actions.mean(axis=0)\n",
    "        self.action_std = all_actions.std(axis=0) + 1e-8\n",
    "        \n",
    "    def normalize_obs(self, obs):\n",
    "        return (obs - self.obs_mean) / self.obs_std\n",
    "    \n",
    "    def normalize_action(self, action):\n",
    "        return (action - self.action_mean) / self.action_std\n",
    "    \n",
    "    def denormalize_action(self, action):\n",
    "        return action * self.action_std + self.action_mean\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ep_idx, start_idx = self.indices[idx]\n",
    "        episode = self.episodes[ep_idx]\n",
    "        \n",
    "        # Extract observation window\n",
    "        obs_seq = episode['observations'][start_idx:start_idx + self.n_obs_steps]\n",
    "        \n",
    "        # Extract action sequence\n",
    "        action_seq = episode['actions'][start_idx:start_idx + self.horizon]\n",
    "        \n",
    "        # Normalize\n",
    "        obs_seq = self.normalize_obs(obs_seq)\n",
    "        action_seq = self.normalize_action(action_seq)\n",
    "        \n",
    "        return {\n",
    "            'obs': torch.FloatTensor(obs_seq),\n",
    "            'action': torch.FloatTensor(action_seq)\n",
    "        }\n",
    "\n",
    "# Load and create dataset\n",
    "with open('lunar_lander_demos.pkl', 'rb') as f:\n",
    "    episodes = pickle.load(f)\n",
    "\n",
    "dataset = LunarLanderDataset(episodes, horizon=horizon, n_obs_steps=n_obs_steps)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5095789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "from diffusion_policy.common.normalize_util import LinearNormalizer\n",
    "\n",
    "# Build model (reuse from your existing code)\n",
    "model = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * n_obs_steps,\n",
    "    local_cond_dim=None,\n",
    "    diffusion_step_embed_dim=256,\n",
    "    down_dims=[256, 512, 1024],\n",
    "    kernel_size=5,\n",
    "    n_groups=8,\n",
    "    cond_predict_scale=True,\n",
    "    horizon=horizon\n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMTEDiScheduler(\n",
    "    num_train_timesteps=num_train_timesteps,\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    variance_type='fixed_small',\n",
    "    clip_sample=True,\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "policy = TEDiUnetLowdimPolicy(\n",
    "    model=model,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    horizon=horizon,\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=action_dim,\n",
    "    n_action_steps=n_action_steps,\n",
    "    n_obs_steps=n_obs_steps,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    obs_as_global_cond=True,\n",
    "    obs_as_local_cond=False,\n",
    "    pred_action_steps_only=False,\n",
    "    temporally_constant_weight=0.2,\n",
    "    temporally_increasing_weight=0.0,\n",
    "    temporally_random_weights=0.0,\n",
    "    chunk_wise_weight=0.8,\n",
    "    buffer_init=\"zero\"\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "policy = policy.to(device)\n",
    "\n",
    "# Setup normalizer\n",
    "normalizer = LinearNormalizer()\n",
    "normalizer['obs'] = LinearNormalizer()\n",
    "normalizer['obs'].fit(torch.from_numpy(np.concatenate([ep['observations'] for ep in episodes])))\n",
    "normalizer['action'] = LinearNormalizer()\n",
    "normalizer['action'].fit(torch.from_numpy(np.concatenate([ep['actions'] for ep in episodes])))\n",
    "policy.set_normalizer(normalizer)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(policy.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    policy.model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Move to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = policy.compute_loss(batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': policy.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, f'lunar_lander_ckpt_epoch_{epoch+1}.pt')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f130b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def evaluate_policy(policy, num_episodes=10, render=True):\n",
    "    \"\"\"Evaluate trained policy on Lunar Lander\"\"\"\n",
    "    env = gym.make('LunarLanderContinuous-v2', render_mode='human' if render else None)\n",
    "    \n",
    "    policy.model.eval()\n",
    "    all_rewards = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        policy.reset_buffer()  # Important: reset for each episode!\n",
    "        \n",
    "        obs_history = deque(maxlen=n_obs_steps)\n",
    "        # Initialize with first observation\n",
    "        for _ in range(n_obs_steps):\n",
    "            obs_history.append(obs)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "        step_count = 0\n",
    "        \n",
    "        while not (done or truncated) and step_count < 1000:\n",
    "            # Prepare observation\n",
    "            obs_seq = np.array(list(obs_history))\n",
    "            obs_tensor = torch.FloatTensor(obs_seq).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Predict actions\n",
    "            with torch.no_grad():\n",
    "                result = policy.predict_action({'obs': obs_tensor})\n",
    "                action_seq = result['action'].cpu().numpy()[0]\n",
    "            \n",
    "            # Execute first action (or multiple if n_action_steps > 1)\n",
    "            for act_idx in range(min(n_action_steps, len(action_seq))):\n",
    "                if done or truncated:\n",
    "                    break\n",
    "                    \n",
    "                action = action_seq[act_idx]\n",
    "                action = np.clip(action, -1, 1)  # Ensure valid action\n",
    "                \n",
    "                obs, reward, done, truncated, info = env.step(action)\n",
    "                obs_history.append(obs)\n",
    "                episode_reward += reward\n",
    "                step_count += 1\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        print(f\"Episode {ep+1}: Reward = {episode_reward:.1f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    mean_reward = np.mean(all_rewards)\n",
    "    std_reward = np.std(all_rewards)\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"   Mean Reward: {mean_reward:.1f} ± {std_reward:.1f}\")\n",
    "    print(f\"   Success Rate: {sum(r > 200 for r in all_rewards) / len(all_rewards) * 100:.1f}%\")\n",
    "    \n",
    "    return all_rewards\n",
    "\n",
    "# Run evaluation\n",
    "rewards = evaluate_policy(policy, num_episodes=5, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
